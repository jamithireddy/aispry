{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "import os\n",
    "from tkinter import filedialog\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=tk.Tk()\n",
    "root.withdraw()\n",
    "file=filedialog.askopenfile(title=\"Select the file\")\n",
    "education = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Moment Business Decision: Mean Median, Mode\n",
    "\n",
    "print('The mean Workexp is {0}, The Median workexp is {1}, The Mode of Workexp is {2}'.format(education.workex.mean(),education.workex.median(),education.workex.mode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Moment Business Decision: The measures of dispersion: Variance, Standard Deviance and Range\n",
    "\n",
    "print('The Variance of the work Experience is {0}, Standard deviation is {1} and range is {2}'.format(education.workex.var(),education.workex.std(),(max(education.workex)-min(education.workex))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Moment Business Decision : Skewness\n",
    "\n",
    "print('The Skewness of Workexp is {0} and the skewness of GMAT is {1}'.format(education.workex.skew(),education.gmat.skew()))\n",
    "\n",
    "# -ve skewness value indicates it is a left tailed or left skewed\n",
    "# +ve skewness indicate it is a right tailed distribution or right skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth Moment of Business: Kurtosis\n",
    "\n",
    "print('The Skewness of Workexp is {0} and the skewness of GMAT is {1}'.format(education.workex.kurt(),education.gmat.kurt()))\n",
    "\n",
    "# Positive value in Kurtosis indicate a narrow peak\n",
    "# Negative valued kurtosis indicate a wider peak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Visualization </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education.shape # Gives the shape of the data. i.e.,  number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(height=education.gmat,x=np.arange(1,774,1)); \n",
    "# By ending the Plot quote with ';' We can elminate the test generated above the plot.\n",
    "# Generating a bar chart with each value in the row as a bar, representing GMAT scores on Y-axis and arranged numbers from 1 to 774 in increment of 1 on X-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(education.gmat);\n",
    "\n",
    "# Histogram of GMAT scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(education.gmat);\n",
    "\n",
    "# Boxplot for the GMAT Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a Quantile Quantile plot to check if the data is normally distributed or not\n",
    "\n",
    "import pylab\n",
    "stats.probplot(education.gmat,dist='norm',plot=pylab) # from scipy stats library was imported already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(education.workex,dist='norm',plot=pylab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data looks like exponential distribution. So, applying Log transformation\n",
    "stats.probplot(np.log(education.workex),dist='norm',plot=pylab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=tk.Tk()\n",
    "root.withdraw()\n",
    "file=filedialog.askopenfile(title=\"Select the file\")\n",
    "# Import ethnic diversity Dataset from EDA_Dataset folder\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Typecasting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the Salaries into int64 format\n",
    "print(df.dtypes)\n",
    "df.Salaries=df.Salaries.astype('int64')\n",
    "print('The updated data  type of Salies is {}'.format(df.Salaries.dtype))\n",
    "a= df.age.dtypes\n",
    "df.age=df.age.astype('float32')\n",
    "print('The initial data type for age is {0}. The updated data type of age now is {1}'.format(a,df.age.dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Identifying and removing duplicates in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=tk.Tk()\n",
    "root.withdraw()\n",
    "file=filedialog.askopenfile(title=\"Select the file\")\n",
    "# Import ethnic mtcars_dup from EDA_Dataset folder\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate =df.duplicated() # Creates a Series filled with True or False if the row is a duplicate or not\n",
    "sum(duplicate) # Returns the total number of duplicates\n",
    "sum(df.cyl.duplicated())# Returns the number of duplicated values in the column cyl\n",
    "## Removing the duplcates ##\n",
    "df =df.drop_duplicates() # Drops the Duplicated rows\n",
    "sum(df.duplicated())# Checking for duplicates after dropping the duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=tk.Tk()\n",
    "root.withdraw()\n",
    "file=filedialog.askopenfile(title=\"Select the file\")\n",
    "# Import ethnic diversity dataset from EDA_Dataset folder\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Findout the outliers in Salaries Column.\n",
    "sns.boxplot(x=df.Salaries);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alpha Trimming ##\n",
    "# A column can be either referred as dataframe[column] or dataframe.column\n",
    "IQR=df['Salaries'].quantile(0.75)-df.Salaries.quantile(0.25) # Calculating the Inter Quantile range Q3-Q1\n",
    "lower_limit=df['Salaries'].quantile(0.25)-(IQR*1.5)\n",
    "upper_limit=df['Salaries'].quantile(0.75)+(IQR*1.5)\n",
    "\n",
    "# Alpha-Trimming\n",
    "outliers=np.where(df.Salaries>upper_limit,True,np.where(df.Salaries<lower_limit,True,False))\n",
    "df_trimmed=df.loc[~(outliers),]\n",
    "print(df.shape)\n",
    "print(df_trimmed.shape)\n",
    "sns.boxplot(x=df_trimmed.Salaries);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Winsorization ##\n",
    "# Replacing the outlier values with the border value i.e., Q3+IQR*1.5 and Q1-IQR*1.5\n",
    "df['Modified_Sal']= pd.DataFrame(np.where(df.Salaries>upper_limit,upper_limit,np.where(df.Salaries<lower_limit,lower_limit,df.Salaries)))\n",
    "print(df.shape)\n",
    "sns.boxplot(x=df.Modified_Sal);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively we can use winsorizer function from feature_engine\n",
    "from feature_engine.outliers import Winsorizer\n",
    "winsor=Winsorizer(capping_method='iqr',\n",
    "                        tail='both',\n",
    "                        fold=1.5,\n",
    "                        variables=['Salaries'])\n",
    "df_t=winsor.fit_transform(df[['Salaries']])\n",
    "sns.boxplot(x=df_t.Salaries);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Zero Variance and Near zero variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.var()\n",
    "# If the Variance of the data in a column is Zero or near zero, we cannot do any statistical analysis>Hence we can ignore that feature for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Missing values imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=tk.Tk()\n",
    "root.withdraw()\n",
    "file=filedialog.askopenfile(title=\"Select the file\")\n",
    "# Import modified ethnic dataset from EDA_Dataset folder\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() # Checking the Missing values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "mean_imputer=SimpleImputer(strategy='mean')\n",
    "df.Salaries=pd.DataFrame(mean_imputer.fit_transform(df[['Salaries']]))\n",
    "print('The Na values in the Salaries column now is {0}'.format(df.Salaries.isna().sum()))\n",
    "\n",
    "# Median imputation\n",
    "median_imputer=SimpleImputer(strategy='median')\n",
    "df['age']=pd.DataFrame(median_imputer.fit_transform(df[['age']]))\n",
    "print('The Na values in the age column now is {0}'.format(df.age.isna().sum()))\n",
    "\n",
    "# Mode imputation\n",
    "mode_imputer=SimpleImputer(strategy='most_frequent')\n",
    "df['Sex']=pd.DataFrame(mode_imputer.fit_transform(df[['Sex']]))\n",
    "print('The Na values in the Sex column now is {0}'.format(df.Sex.isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Dummy variable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=tk.Tk()\n",
    "root.withdraw()\n",
    "file=filedialog.askopenfile(title=\"Select the file\")\n",
    "# Import ethnic diversity dataset from EDA_Dataset folder\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns # Gives all the column names\n",
    "df.shape  # Gives the number of rows and no.of columns in a dataframe.\n",
    "df.drop(['Employee_Name','EmpID','Zip'],axis=1,inplace=True) # Dropping specified columns. Inplace =True means in memory only\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dummy variables for all categorical columns\n",
    "df_new=pd.get_dummies(df)\n",
    "df_new1=pd.get_dummies(df,drop_first=True)# creating dummies after dropping the first category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One hot Encoding ##\n",
    "df=df[['Salaries','age','Position','State','Sex','MaritalDesc','CitizenDesc','EmploymentStatus','Department','Race']] #Creating a df with selected columns in specific order.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc=OneHotEncoder()  # Initializing the method with default arguements\n",
    "enc_df=pd.DataFrame(enc.fit_transform(df.iloc[:,2:]).toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Label Encoding ##\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label= LabelEncoder() # Initializing the method with default arguements\n",
    "# Splitting the data into Output and Input Variables\n",
    "# The splitting is optional and not mandatory in data cleaning. But we do this for Machine Learning\n",
    "X=df.iloc[:,:9]\n",
    "y=df['Race']\n",
    "X['Sex']=label.fit_transform(X['Sex'])\n",
    "X['MaritalDesc']=label.fit_transform(X['MaritalDesc'])\n",
    "X['CitizenDesc']=label.fit_transform(X['CitizenDesc'])\n",
    "y=label.fit_transform(y)# Creates an array of numbers\n",
    "y=pd.DataFrame(y)\n",
    "df_new2=pd.concat([X,y],axis=1)# Appends the y column as column name'0' in the end\n",
    "df_new2=df_new2.rename(columns={0:'Type'}) # Renaming the y column as 'Type'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Standardization and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=tk.Tk()\n",
    "root.withdraw()\n",
    "file=filedialog.askopenfile(title=\"Select the file\")\n",
    "# Import mtcars dataset from EDA_Dataset folder\n",
    "df = pd.read_csv(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()# Initializing the scaler with default arguements\n",
    "data=pd.DataFrame(scaler.fit_transform(df))\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "root=tk.Tk()\n",
    "root.withdraw()\n",
    "file=filedialog.askopenfile(title=\"Select the file\")\n",
    "# Import Ethinc diversity dataset from EDA_Dataset folder\n",
    "df = pd.read_csv(file)\n",
    "df.drop(['Employee_Name','EmpID','Zip'],axis=1,inplace=True)\n",
    "df=pd.get_dummies(df,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Writing a custom function to calculate the Normalized values\n",
    "def norm_func(i):\n",
    "  x=(i-i.min())/(i.max()-i.min())\n",
    "  return(x)\n",
    "\n",
    "df_norm=norm_func(df)\n",
    "df_norm.describe()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8c4a8b360c0157dfe26207fb9fdfcde646f5420e94cd86b887b553c73308aa1b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
